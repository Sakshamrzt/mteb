{
  "dataset_revision": "308230fabdcada106869a5196ba11c403d065b07",
  "mteb_dataset_name": "IndicNLPNewsClassification",
  "mteb_version": "1.7.50",
  "test": {
    "evaluation_time": 98.9,
    "gu": {
      "accuracy": 0.949609375,
      "accuracy_stderr": 0.011247015663193726,
      "f1": 0.9490335972955988,
      "f1_stderr": 0.011678355597864723,
      "main_score": 0.949609375
    },
    "kn": {
      "accuracy": 0.95546875,
      "accuracy_stderr": 0.008593749999999999,
      "f1": 0.9553988439788507,
      "f1_stderr": 0.008620877041113336,
      "main_score": 0.95546875
    },
    "mal": {
      "accuracy": 0.829296875,
      "accuracy_stderr": 0.018819046047571723,
      "f1": 0.8253330840820441,
      "f1_stderr": 0.020243289039823587,
      "main_score": 0.829296875
    },
    "mr": {
      "accuracy": 0.973046875,
      "accuracy_stderr": 0.006866560871580838,
      "f1": 0.9729359616511681,
      "f1_stderr": 0.006891540530198213,
      "main_score": 0.973046875
    },
    "ori": {
      "accuracy": 0.94765625,
      "accuracy_stderr": 0.01121304694875572,
      "f1": 0.9474153408968732,
      "f1_stderr": 0.011244396665799338,
      "main_score": 0.94765625
    },
    "pa": {
      "accuracy": 0.953515625,
      "accuracy_stderr": 0.01164691524677779,
      "f1": 0.954088692824141,
      "f1_stderr": 0.01162796143856771,
      "main_score": 0.953515625
    },
    "ta": {
      "accuracy": 0.944921875,
      "accuracy_stderr": 0.007297477223542736,
      "f1": 0.9451132289988475,
      "f1_stderr": 0.007273541856628014,
      "main_score": 0.944921875
    },
    "tel": {
      "accuracy": 0.970703125,
      "accuracy_stderr": 0.014111243647022221,
      "f1": 0.9708007477867653,
      "f1_stderr": 0.014098394867351653,
      "main_score": 0.970703125
    }
  }
}